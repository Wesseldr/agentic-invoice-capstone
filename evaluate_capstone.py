# ============================================================================
# File: evaluate_capstone.py
# Author: J.W. de Roode
# GitHub: https://github.com/Wesseldr
# Project: Agentic Invoice Processor ‚Äì Google AI 5-Day Intensive
# Created: 2025-11-30
# Description:
#     Definitive evaluation module. Compares agent-generated JSON with ground
#     truth and produces a strict accuracy report for the Capstone submission.
# ============================================================================

"""
Evaluation Module
=================

This script performs the definitive quality assessment of the Agentic Invoice Processor.
It compares the JSON output generated by the Agents against a manually verified "Ground Truth" dataset.

Key Responsibilities:
1. **Matching:** Locating the generated output file for each ground truth file.
2. **Scoring:** Comparing critical fields (Invoice Number, Date, Supplier) and line items using strict equality checks.
3. **Reporting:** Calculating a final accuracy percentage (0-100%) and listing specific discrepancies.

Usage:
    Run this script from the project root to evaluate the latest batch of processed invoices.
    $ python evaluate_capstone.py
"""
import json
from pathlib import Path
from deepdiff import DeepDiff  # pip install deepdiff

def load_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

# In src/DevTools/evaluate_capstone.py

def evaluate_invoice(generated_path, ground_truth_path):
    """
    Compares a single generated JSON against its Ground Truth counterpart.
    
    Scoring Logic:
    - Header Fields (5 points): Invoice Number, Date, Supplier, KvK, VAT.
    - Line Items (1 point): The set of validatedClientCaseNumbers must match exactly.
    - No Activity (1 point): The set of non-billable cases must match exactly.
    
    Args:
        generated_path (Path): Path to the JSON produced by the Agent.
        ground_truth_path (Path): Path to the verified 'Answer Key' JSON.
        
    Returns:
        tuple: (accuracy_percentage, report_list)
    """
    gen = load_json(generated_path)
    truth = load_json(ground_truth_path)
    
    score = 0
    total_points = 0
    report = []

    # 1. Header Checks (Critical)
    fields = ["invoiceNumber", "invoiceDate", "supplierName", "kvkNumber", "vatNumber"]
    for field in fields:
        total_points += 1
        val_gen = gen["invoiceHeader"].get(field)
        val_truth = truth["invoiceHeader"].get(field)
        
        # Fuzzy match for supplier name (case insensitive)
        if field == "supplierName" and val_gen and val_truth:
            if val_truth.lower() in val_gen.lower() or val_gen.lower() in val_truth.lower():
                score += 1
                continue
                
        if val_gen == val_truth:
            score += 1
        else:
            report.append(f"‚ùå {field} mismatch: Expected '{val_truth}', got '{val_gen}'")

    # 2. Line Item Logic (Set Comparison)
    # --- FIX: GEBRUIK NIEUWE VELDNAAM VOOR VERGELIJKING ---
    gen_cases = set(c["validatedClientCaseNumber"] for c in gen["clientCases"])
    truth_cases = set(c["validatedClientCaseNumber"] for c in truth["clientCases"])
    # ----------------------------------------------------
    
    total_points += 1
    if gen_cases == truth_cases:
        score += 1
    else:
        missing = truth_cases - gen_cases
        extra = gen_cases - truth_cases
        report.append(f"‚ùå Line Items mismatch. Missing: {missing}, Unexpected: {extra}")

    # 3. No Activity Logic
    # Deze gebruikt al strings, dus dit is prima:
    gen_no_act = set(gen["clientCasesNoActivity"])
    truth_no_act = set(truth["clientCasesNoActivity"])
    
    total_points += 1
    if gen_no_act == truth_no_act:
        score += 1
    else:
        report.append(f"‚ùå NoActivity mismatch.")

    accuracy = (score / total_points) * 100
    return accuracy, report

def run_evaluation():
    """
    Main execution loop.
    Iterates through all Ground Truth files and looks for matching Agent outputs.
    Prints a detailed report per invoice and a final average score.
    """
    repo_root = Path(__file__).resolve().parent
    
    gen_dir = repo_root / "data/llm_ready/json_out_multi_agent"
    truth_dir = repo_root / "data/evaluation/ground_truth"
        
    print("üìä STARTING CAPSTONE EVALUATION\n")
    
    results = []
    
    # --- FIX 1: Sorteer de bestanden ---
    truth_files = sorted(list(truth_dir.glob("*.json")))
    
    for truth_file in truth_files:
        # --- FIX 2: Correcte bestandsnaam afleiden ---
        # Ground Truth: "Factuur 001.json" -> Generated: "Factuur 001_parsed.json"
        generated_name = truth_file.name.replace(".json", "_parsed.json")
        gen_file = gen_dir / generated_name
        
        if not gen_file.exists():
            print(f"‚ö†Ô∏è Missing generated file for `{truth_file.name}`. Tried looking for: `{generated_name}`")
            continue
            
        acc, report = evaluate_invoice(gen_file, truth_file)
        results.append(acc)
        
        print(f"üìÑ {truth_file.name}: {acc:.1f}%")
        for line in report:
            print(f"   {line}")
            
    if results:
        avg_score = sum(results) / len(results)
        print(f"\n‚≠ê FINAL SCORE: {avg_score:.1f}%")
    else:
        print("No files evaluated.")

if __name__ == "__main__":
    run_evaluation()